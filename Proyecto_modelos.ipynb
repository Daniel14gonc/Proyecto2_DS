{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto 2 modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pydicom\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from scipy import ndimage\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from zipfile import ZipFile\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_VGG = pd.read_csv('train_filtrado.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StudyInstanceUID</th>\n",
       "      <th>patient_overall</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.2.826.0.1.3680043.6200</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.2.826.0.1.3680043.27262</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.2.826.0.1.3680043.21561</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.2.826.0.1.3680043.12351</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.2.826.0.1.3680043.1363</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            StudyInstanceUID  patient_overall  C1  C2  C3  C4  C5  C6  C7\n",
       "0   1.2.826.0.1.3680043.6200                1   1   1   0   0   0   0   0\n",
       "1  1.2.826.0.1.3680043.27262                1   0   1   0   0   0   0   0\n",
       "2  1.2.826.0.1.3680043.21561                1   0   1   0   0   0   0   0\n",
       "3  1.2.826.0.1.3680043.12351                0   0   0   0   0   0   0   0\n",
       "4   1.2.826.0.1.3680043.1363                1   0   0   0   0   1   0   0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_VGG.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros del modelo\n",
    "vgg_output_size = 512  # Tamaño de la salida de la VGG16\n",
    "gru_hidden_size = 128  # Tamaño del estado oculto de la GRU\n",
    "gru_num_layers = 2  # Número de capas en la GRU\n",
    "num_classes = 7  # Reemplaza con el número de clases en tu problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedMultiLabelLogLoss(nn.Module):\n",
    "    def __init__(self, weight=None):\n",
    "        super(WeightedMultiLabelLogLoss, self).__init__()\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        Computes the weighted multi-label logarithmic loss.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): Predicted probabilities (output of the model).\n",
    "                                Shape: (batch_size, num_classes)\n",
    "            target (torch.Tensor): Target labels (ground truth).\n",
    "                                Shape: (batch_size, num_classes)\n",
    "\n",
    "        Returns:\n",
    "            loss (torch.Tensor): Weighted multi-label logarithmic loss.\n",
    "        \"\"\"\n",
    "        epsilon = 1e-15  # Small constant to avoid log(0)\n",
    "\n",
    "        # Log loss\n",
    "        log_loss = -target * torch.log(input + epsilon) - (1 - target) * torch.log(1 - input + epsilon)\n",
    "\n",
    "        # Apply weights if provided\n",
    "        if self.weight is not None:\n",
    "            log_loss = log_loss * self.weight\n",
    "\n",
    "        # Compute mean loss over samples and classes\n",
    "        loss = log_loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# Define la arquitectura del modelo combinado\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, vgg_output_size, gru_hidden_size, gru_num_layers, num_classes):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.vgg16 = models.vgg16(pretrained=True)\n",
    "        self.vgg16.features[28] = nn.AvgPool2d(kernel_size=7, stride=1)  # Cambia el stride de la última capa de pooling\n",
    "        self.vgg16.classifier = nn.Identity()  # Desactiva las capas clasificadoras\n",
    "\n",
    "        self.gru = nn.GRU(vgg_output_size, gru_hidden_size, num_layers=gru_num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(gru_hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pasa cada imagen a través de VGG16 y obtén los feature maps\n",
    "        vgg_outputs = []\n",
    "        for image in x:\n",
    "            vgg_output = self.vgg16(image)\n",
    "            vgg_output = vgg_output.view(vgg_output.size(0), -1, vgg_output.size(1))\n",
    "            vgg_outputs.append(vgg_output)\n",
    "\n",
    "        # Concatena los feature maps de las imágenes a lo largo de la dimensión del tiempo (secuencia)\n",
    "        gru_input = torch.cat(vgg_outputs, dim=0)  # Concatena en la dimensión 0 (batch_size x num_images, 512, height, width)\n",
    "\n",
    "        # Reorganiza la entrada para que cada imagen sea una \"secuencia\" para la GRU\n",
    "        gru_input = gru_input.view(-1, len(vgg_outputs), vgg_output_size)  # (batch_size, num_images, 512)\n",
    "\n",
    "        # Pasa la secuencia de entrada a través de GRU\n",
    "        gru_output, _ = self.gru(gru_input)\n",
    "\n",
    "        # Pasa la última salida de la GRU a través de una capa completamente conectada\n",
    "        output = self.fc(gru_output[:, -1, :])\n",
    "        print(\"pase\")\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define la arquitectura del modelo combinado con EfficientNet\n",
    "# class CombinedModel(nn.Module):\n",
    "#     def __init__(self, efficientnet_model, gru_hidden_size, gru_num_layers, num_classes):\n",
    "#         super(CombinedModel, self).__init__()\n",
    "#         self.efficientnet = efficientnet_model\n",
    "#         self.efficientnet._avg_pooling = nn.AdaptiveAvgPool2d(1)  # Cambia el pooling a adaptativo para cualquier tamaño de entrada\n",
    "#         self.efficientnet._fc = nn.Identity()  # Desactiva las capas completamente conectadas\n",
    "\n",
    "#         # Obtén el tamaño de salida de EfficientNet (depende de la versión)\n",
    "#         # Asume que la última convolución es 2D\n",
    "#         dummy_input = torch.randn(1, 3, 224, 224)  # Suponiendo imágenes de 224x224\n",
    "#         dummy_output = self.efficientnet.extract_features(dummy_input)\n",
    "#         self.efficientnet_output_size = dummy_output.shape[1]\n",
    "\n",
    "#         self.gru = nn.GRU(self.efficientnet_output_size, gru_hidden_size, num_layers=gru_num_layers, batch_first=True)\n",
    "#         self.fc = nn.Linear(gru_hidden_size, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         efficientnet_outputs = []\n",
    "#         for image in x:\n",
    "#             efficientnet_output = self.efficientnet(image)\n",
    "#             efficientnet_output = efficientnet_output.view(efficientnet_output.size(0), -1, efficientnet_output.size(1))\n",
    "#             efficientnet_outputs.append(efficientnet_output)\n",
    "#         # Pasa cada imagen a través de EfficientNet y obtén los feature maps\n",
    "#         efficientnet_output = self.efficientnet.extract_features(x)\n",
    "#         efficientnet_output = efficientnet_output.view(efficientnet_output.size(0), -1, self.efficientnet_output_size)\n",
    "\n",
    "#         # Pasa los feature maps de las imágenes a lo largo de la dimensión del tiempo (secuencia)\n",
    "#         gru_input = efficientnet_output\n",
    "\n",
    "#         # Pasa la secuencia de entrada a través de GRU\n",
    "#         gru_output, _ = self.gru(gru_input)\n",
    "\n",
    "#         # Pasa la última salida de la GRU a través de una capa completamente conectada\n",
    "#         output = self.fc(gru_output[:, -1, :])\n",
    "#         return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU está disponible\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('GPU está disponible')\n",
    "else:\n",
    "    print('No se encontró GPU, usando CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniel\\Main\\UVG\\Semestre VIII\\Data science\\Proyecto2\\myenv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Daniel\\Main\\UVG\\Semestre VIII\\Data science\\Proyecto2\\myenv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Crea una instancia del modelo combinado\n",
    "combined_model = CombinedModel(vgg_output_size, gru_hidden_size, gru_num_layers, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cargar el modelo EfficientNet preentrenado (por ejemplo, EfficientNet-B0)\n",
    "# efficientnet_model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "\n",
    "# # Cambiar el modelo combinado para usar EfficientNet\n",
    "# combined_model = CombinedModel(efficientnet_model, gru_hidden_size,\n",
    "#                                 gru_num_layers, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CombinedModel(\n",
       "  (vgg16): VGG(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): ReLU(inplace=True)\n",
       "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (25): ReLU(inplace=True)\n",
       "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (27): ReLU(inplace=True)\n",
       "      (28): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "    (classifier): Identity()\n",
       "  )\n",
       "  (gru): GRU(512, 128, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define la función para preprocesar las imágenes para VGG16 y CombinedModel\n",
    "def preprocess_image_for_combined_model(image):\n",
    "    # Transformaciones para preprocesar las imágenes para VGG16\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Cambiar el tamaño a 224x224 (tamaño de entrada de la VGG16)\n",
    "        transforms.ToTensor(),  # Convertir a tensor\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalización requerida por VGG16\n",
    "    ])\n",
    "\n",
    "    preprocessed_image = transform(image)  # Aplica las transformaciones\n",
    "    return preprocessed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataGenerator:\n",
    "    def __init__(self, df, ct_folder, batch_size=32):\n",
    "        self.df = df\n",
    "        self.ct_folder = ct_folder\n",
    "        self.batch_size = batch_size\n",
    "        self.num_samples = len(df)\n",
    "        self.current_idx = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "\n",
    "        for _ in range(self.batch_size):\n",
    "            if self.current_idx >= self.num_samples:\n",
    "                self.current_idx = 0\n",
    "                raise StopIteration\n",
    "\n",
    "            ct_name = self.df.iloc[self.current_idx]['StudyInstanceUID']\n",
    "            labels = self.df.iloc[self.current_idx][['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7']].tolist()\n",
    "\n",
    "            with ZipFile(os.path.join(self.ct_folder, ct_name + '.zip'), 'r') as zip_ref:\n",
    "                image_files = zip_ref.namelist()\n",
    "\n",
    "                # Read and process each image\n",
    "                ct_images = []\n",
    "                for image_file in image_files:\n",
    "                    with zip_ref.open(image_file) as img_file:\n",
    "                        image = Image.open(img_file)  # Load the image\n",
    "                        image = preprocess_image_for_combined_model(image)\n",
    "                        ct_images.append(image)\n",
    "\n",
    "                # Append images and labels to the batch\n",
    "                batch_images.append(ct_images)\n",
    "                batch_labels.append(labels)\n",
    "\n",
    "            self.current_idx += 1\n",
    "\n",
    "        \n",
    "        return np.array(batch_images), np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(data_VGG, 'imagenes_train', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Especifica la carpeta donde se almacenarán los registros de TensorBoard\n",
    "log_dir = \"logs\"\n",
    "\n",
    "# Inicializa TensorBoard\n",
    "writer = SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pase\n",
      "pase\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Daniel\\Main\\UVG\\Semestre VIII\\Data science\\Proyecto2\\Proyecto_modelos.ipynb Cell 18\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Daniel/Main/UVG/Semestre%20VIII/Data%20science/Proyecto2/Proyecto_modelos.ipynb#X24sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# Acumular la pérdida total\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Daniel/Main/UVG/Semestre%20VIII/Data%20science/Proyecto2/Proyecto_modelos.ipynb#X24sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Daniel/Main/UVG/Semestre%20VIII/Data%20science/Proyecto2/Proyecto_modelos.ipynb#X24sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Daniel/Main/UVG/Semestre%20VIII/Data%20science/Proyecto2/Proyecto_modelos.ipynb#X24sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# Actualizar la pérdida en tiempo real en TensorBoard\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Daniel/Main/UVG/Semestre%20VIII/Data%20science/Proyecto2/Proyecto_modelos.ipynb#X24sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m display_step \u001b[39m==\u001b[39m display_step \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hiperparámetros de entrenamiento\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5  # Número de épocas (iteraciones completas sobre el conjunto de datos)\n",
    "display_step = 10\n",
    "# Suponiendo que `data_generator` es la instancia del generador de datos que creamos antes\n",
    "\n",
    "# Definir la función de pérdida y el optimizador\n",
    "criterion = WeightedMultiLabelLogLoss()  # Suponiendo una tarea de clasificación\n",
    "optimizer = optim.Adam(combined_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "combined_model.train()  # Poner el modelo en modo de entrenamiento\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "    # Iterar a través del generador en mini lotes\n",
    "    for i, (images_batch, labels_batch) in enumerate(data_generator):\n",
    "        # Convertir el lote de imágenes a tensores y pasarlos por el modelo\n",
    "        images_tensor = torch.tensor(images_batch, dtype=torch.float32).to('cuda')\n",
    "        labels_tensor = torch.tensor(labels_batch, dtype=torch.long).to('cuda')  # Usar torch.long para las etiquetas\n",
    "        print(\"pase\")\n",
    "        # Realizar la propagación hacia adelante (forward pass)\n",
    "        predictions = combined_model(images_tensor)\n",
    "\n",
    "        loss = criterion(predictions, labels_tensor)\n",
    "\n",
    "        # Realizar la retropropagación y la optimización\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Acumular la pérdida total\n",
    "        total_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Actualizar la pérdida en tiempo real en TensorBoard\n",
    "        if i % display_step == display_step - 1:\n",
    "            avg_loss = running_loss / display_step\n",
    "            writer.add_scalar('Loss', avg_loss, epoch * len(data_generator) + i)\n",
    "\n",
    "            running_loss = 0.0\n",
    "        counter += 1\n",
    "        if counter == 100:\n",
    "            break\n",
    "\n",
    "    # Calcular la pérdida promedio para la época\n",
    "    average_loss = total_loss / len(data_generator)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss:.4f}')\n",
    "\n",
    "print('Entrenamiento completado.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supongamos que ya has entrenado el modelo y deseas guardarlo\n",
    "torch.save(combined_model.state_dict(), 'modelo_entrenado.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo entrenado\n",
    "combined_model = CombinedModel(vgg_output_size, gru_hidden_size, gru_num_layers, num_classes)\n",
    "combined_model.load_state_dict(torch.load('modelo_entrenado.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
