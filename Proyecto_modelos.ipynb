{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto 2 modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pydicom\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from scipy import ndimage\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniel\\Main\\UVG\\Semestre VIII\\Data science\\Proyecto2\\myenv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Daniel\\Main\\UVG\\Semestre VIII\\Data science\\Proyecto2\\myenv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# Define la arquitectura del modelo combinado\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, vgg_output_size, gru_hidden_size, gru_num_layers, num_classes):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.vgg16 = models.vgg16(pretrained=True)\n",
    "        self.vgg16.features[28] = nn.AvgPool2d(kernel_size=7, stride=1)  # Cambia el stride de la última capa de pooling\n",
    "        self.vgg16.classifier = nn.Identity()  # Desactiva las capas clasificadoras\n",
    "\n",
    "        self.gru = nn.GRU(vgg_output_size, gru_hidden_size, num_layers=gru_num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(gru_hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pasa cada imagen a través de VGG16 y obtén los feature maps\n",
    "        vgg_outputs = []\n",
    "        for image in x:\n",
    "            vgg_output = self.vgg16(image)\n",
    "            vgg_output = vgg_output.view(vgg_output.size(0), -1, vgg_output.size(1))\n",
    "            vgg_outputs.append(vgg_output)\n",
    "        \n",
    "        # Concatena los feature maps de las imágenes a lo largo de la dimensión del tiempo (secuencia)\n",
    "        gru_input = torch.cat(vgg_outputs, dim=0)  # Concatena en la dimensión 0 (batch_size x num_images, 512, height, width)\n",
    "\n",
    "        # Reorganiza la entrada para que cada imagen sea una \"secuencia\" para la GRU\n",
    "        gru_input = gru_input.view(-1, len(vgg_outputs), vgg_output_size)  # (batch_size, num_images, 512)\n",
    "\n",
    "        # Pasa la secuencia de entrada a través de GRU\n",
    "        gru_output, _ = self.gru(gru_input)\n",
    "\n",
    "        # Pasa la última salida de la GRU a través de una capa completamente conectada\n",
    "        output = self.fc(gru_output[:, -1, :])\n",
    "        return output\n",
    "\n",
    "# Parámetros del modelo\n",
    "vgg_output_size = 512  # Tamaño de la salida de la VGG16\n",
    "gru_hidden_size = 128  # Tamaño del estado oculto de la GRU\n",
    "gru_num_layers = 2  # Número de capas en la GRU\n",
    "num_classes = 7  # Reemplaza con el número de clases en tu problema\n",
    "\n",
    "# Crea una instancia del modelo combinado\n",
    "model_combined = CombinedModel(vgg_output_size, gru_hidden_size, gru_num_layers, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagenes preprocesadas para VGG16: torch.Size([50, 3, 224, 224])\n",
      "Salida del modelo CombinedModel: torch.Size([49, 7])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Define la función para preprocesar las imágenes para VGG16 y CombinedModel\n",
    "def preprocess_images_for_combined_model(image_set):\n",
    "    # Transformaciones para preprocesar las imágenes para VGG16\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),  # Convertir a objeto PIL (es necesario para la VGG16)\n",
    "        transforms.Resize((224, 224)),  # Cambiar el tamaño a 224x224 (tamaño de entrada de la VGG16)\n",
    "        transforms.ToTensor(),  # Convertir a tensor\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalización requerida por VGG16\n",
    "    ])\n",
    "\n",
    "    preprocessed_images = []\n",
    "    cont = 0\n",
    "    for image in image_set:\n",
    "        preprocessed_image = transform(image)  # Aplica las transformaciones\n",
    "        preprocessed_images.append(preprocessed_image)\n",
    "        cont += 1\n",
    "        if cont == 50:\n",
    "            break\n",
    "    return torch.stack(preprocessed_images)  # Apila las imágenes en un tensor\n",
    "\n",
    "# Preprocesa las imágenes para VGG16 y CombinedModel\n",
    "preprocessed_images_tensor = preprocess_images_for_combined_model(image_set)\n",
    "print(f'Imagenes preprocesadas para VGG16: {preprocessed_images_tensor.shape}')\n",
    "\n",
    "# Ahora, puedes pasar estas imágenes preprocesadas al modelo CombinedModel\n",
    "output = model_combined(preprocessed_images_tensor)\n",
    "print(f'Salida del modelo CombinedModel: {output.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images_from_folder(folder_path):\n",
    "    image_set = []  # Lista para almacenar las imágenes\n",
    "\n",
    "    # Obtén la lista de archivos en la carpeta\n",
    "    files = os.listdir(folder_path)\n",
    "\n",
    "    for file in files:\n",
    "        # Verifica si el archivo es una imagen\n",
    "        if file.endswith(('png', 'jpg', 'jpeg', 'gif')):\n",
    "            # Carga la imagen y conviértela a un arreglo numpy\n",
    "            image_path = os.path.join(folder_path, file)\n",
    "            image = Image.open(image_path)\n",
    "            image = np.array(image)\n",
    "\n",
    "            # Agrega la imagen al conjunto\n",
    "            image_set.append(image)\n",
    "\n",
    "    return np.array(image_set)\n",
    "\n",
    "# Ruta de la carpeta que contiene las imágenes\n",
    "folder_path = \"imagenes\"\n",
    "\n",
    "# Lee las imágenes desde la carpeta y obtén el conjunto de imágenes\n",
    "image_set = read_images_from_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagenes: torch.Size([1, 436, 512, 512, 3])\n",
      "torch.Size([436, 512, 512, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 3, 3], expected input[436, 512, 512, 3] to have 3 channels, but got 512 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Daniel\\Main\\UVG\\Semestre VIII\\Data science\\Proyecto2\\Proyecto_modelos.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Daniel/Main/UVG/Semestre%20VIII/Data%20science/Proyecto2/Proyecto_modelos.ipynb#W4sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m model_combined\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Daniel/Main/UVG/Semestre%20VIII/Data%20science/Proyecto2/Proyecto_modelos.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Daniel/Main/UVG/Semestre%20VIII/Data%20science/Proyecto2/Proyecto_modelos.ipynb#W4sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m outputs \u001b[39m=\u001b[39m model_combined(images_tensor)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Daniel/Main/UVG/Semestre%20VIII/Data%20science/Proyecto2/Proyecto_modelos.ipynb#W4sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, label_tensor\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m))  \u001b[39m# Se agrega una dimensión para la etiqueta\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Daniel/Main/UVG/Semestre%20VIII/Data%20science/Proyecto2/Proyecto_modelos.ipynb#W4sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Main\\UVG\\Semestre VIII\\Data science\\Proyecto2\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\Daniel\\Main\\UVG\\Semestre VIII\\Data science\\Proyecto2\\Proyecto_modelos.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Daniel/Main/UVG/Semestre%20VIII/Data%20science/Proyecto2/Proyecto_modelos.ipynb#W4sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m x:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Daniel/Main/UVG/Semestre%20VIII/Data%20science/Proyecto2/Proyecto_modelos.ipynb#W4sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mprint\u001b[39m(image\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Daniel/Main/UVG/Semestre%20VIII/Data%20science/Proyecto2/Proyecto_modelos.ipynb#W4sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     vgg_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvgg16(image)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Daniel/Main/UVG/Semestre%20VIII/Data%20science/Proyecto2/Proyecto_modelos.ipynb#W4sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     vgg_output \u001b[39m=\u001b[39m vgg_output\u001b[39m.\u001b[39mview(vgg_output\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, vgg_output\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Daniel/Main/UVG/Semestre%20VIII/Data%20science/Proyecto2/Proyecto_modelos.ipynb#W4sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     vgg_outputs\u001b[39m.\u001b[39mappend(vgg_output)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Main\\UVG\\Semestre VIII\\Data science\\Proyecto2\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Main\\UVG\\Semestre VIII\\Data science\\Proyecto2\\myenv\\lib\\site-packages\\torchvision\\models\\vgg.py:66\u001b[0m, in \u001b[0;36mVGG.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m---> 66\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures(x)\n\u001b[0;32m     67\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[0;32m     68\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Main\\UVG\\Semestre VIII\\Data science\\Proyecto2\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Main\\UVG\\Semestre VIII\\Data science\\Proyecto2\\myenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Main\\UVG\\Semestre VIII\\Data science\\Proyecto2\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Main\\UVG\\Semestre VIII\\Data science\\Proyecto2\\myenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Main\\UVG\\Semestre VIII\\Data science\\Proyecto2\\myenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 3, 3], expected input[436, 512, 512, 3] to have 3 channels, but got 512 channels instead"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Crear una muestra de entrenamiento con el conjunto de imágenes y la etiqueta\n",
    "train_sample = {'image_set': image_set, 'label': 1}\n",
    "\n",
    "# Convertir las imágenes a tensores de PyTorch y agregar una dimensión adicional para representar el batch size (1 en este caso)\n",
    "images_tensor = torch.tensor(train_sample['image_set'], dtype=torch.float32).unsqueeze(0)\n",
    "print(f'Imagenes: {images_tensor.shape}')\n",
    "\n",
    "# Convertir la etiqueta a tensor de PyTorch\n",
    "label_tensor = torch.tensor(train_sample['label'], dtype=torch.long)\n",
    "\n",
    "# Define el modelo, la función de pérdida y el optimizador\n",
    "model_combined = CombinedModel(vgg_output_size, gru_hidden_size, gru_num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_combined.parameters(), lr=0.001)\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "model_combined.train()\n",
    "optimizer.zero_grad()\n",
    "outputs = model_combined(images_tensor)\n",
    "loss = criterion(outputs, label_tensor.unsqueeze(0))  # Se agrega una dimensión para la etiqueta\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(f'Loss: {loss.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
